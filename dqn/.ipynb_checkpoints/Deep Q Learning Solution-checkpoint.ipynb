{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-28 09:21:32,093] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari iamges. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.train.SummaryWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 RGB frames of shape 160, 160 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calcualte the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.merge_summary([\n",
    "            tf.scalar_summary(\"loss\", self.loss),\n",
    "            tf.histogram_summary(\"loss_hist\", self.losses),\n",
    "            tf.histogram_summary(\"q_values_hist\", self.predictions),\n",
    "            tf.scalar_summary(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.02124935  0.        ]\n",
      " [ 0.          0.          0.02124935  0.        ]]\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' with argument inside generator (<ipython-input-20-b9874b2ff125>, line 189)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-b9874b2ff125>\"\u001b[0;36m, line \u001b[0;32m189\u001b[0m\n\u001b[0;31m    return stats\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' with argument inside generator\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=50000,\n",
    "                    replay_memory_init_size=5000,\n",
    "                    update_target_estimator_every=1000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=50000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for fff-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Lambda time discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[total_t])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    # Record videos\n",
    "    env.monitor.start(monitor_path,\n",
    "                      resume=True,\n",
    "                      video_callable=lambda count: count % record_video_every == 0)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\" \")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    env.monitor.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoint /home/sk/drl_sk/reinforcement-learning-master/DQN/experiments/Breakout-v0/checkpoints/model...\n",
      "\n",
      "Populating replay memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-28 09:32:04,847] Starting new video recorder writing to /home/sk/drl_sk/reinforcement-learning-master/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.2877.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copied model parameters to target network.\n",
      "Step 0 (0) @ Episode 1/100, loss: None\n",
      "Step 1 (1) @ Episode 1/100, loss: 0.000638611847535\n",
      "Step 2 (2) @ Episode 1/100, loss: 0.000660089892335\n",
      "Step 3 (3) @ Episode 1/100, loss: 0.000631992239505\n",
      "Step 4 (4) @ Episode 1/100, loss: 0.00067826139275\n",
      "Step 5 (5) @ Episode 1/100, loss: 0.033758174628\n",
      "Step 6 (6) @ Episode 1/100, loss: 0.000565850990824\n",
      "Step 7 (7) @ Episode 1/100, loss: 0.000649808265734\n",
      "Step 8 (8) @ Episode 1/100, loss: 0.00065100233769\n",
      "Step 9 (9) @ Episode 1/100, loss: 0.000534773513209\n",
      "Step 10 (10) @ Episode 1/100, loss: 0.000690909859259\n",
      "Step 11 (11) @ Episode 1/100, loss: 0.0337273031473\n",
      "Step 12 (12) @ Episode 1/100, loss: 0.00060751195997\n",
      "Step 13 (13) @ Episode 1/100, loss: 0.000606916379184\n",
      "Step 14 (14) @ Episode 1/100, loss: 0.000665602972731\n",
      "Step 15 (15) @ Episode 1/100, loss: 0.000590139999986\n",
      "Step 16 (16) @ Episode 1/100, loss: 0.000622887746431\n",
      "Step 17 (17) @ Episode 1/100, loss: 0.00066923542181\n",
      "Step 18 (18) @ Episode 1/100, loss: 0.000590149371419\n",
      "Step 19 (19) @ Episode 1/100, loss: 0.000513940234669\n",
      "Step 20 (20) @ Episode 1/100, loss: 0.000661317375489\n",
      "Step 21 (21) @ Episode 1/100, loss: 0.00063711963594\n",
      "Step 22 (22) @ Episode 1/100, loss: 0.000642227358185\n",
      "Step 23 (23) @ Episode 1/100, loss: 0.000635251402855\n",
      "Step 24 (24) @ Episode 1/100, loss: 0.000639161095023\n",
      "Step 25 (25) @ Episode 1/100, loss: 0.0319962315261\n",
      "Step 26 (26) @ Episode 1/100, loss: 0.000583077489864\n",
      "Step 27 (27) @ Episode 1/100, loss: 0.000724573619664\n",
      "Step 28 (28) @ Episode 1/100, loss: 0.000657882890664\n",
      "Step 29 (29) @ Episode 1/100, loss: 0.000563864829019\n",
      "Step 30 (30) @ Episode 1/100, loss: 0.000592408818193\n",
      "Step 31 (31) @ Episode 1/100, loss: 0.000517777109053\n",
      "Step 32 (32) @ Episode 1/100, loss: 0.000580802909099\n",
      "Step 33 (33) @ Episode 1/100, loss: 0.000609729147982\n",
      "Step 34 (34) @ Episode 1/100, loss: 0.000625473156106\n",
      "Step 35 (35) @ Episode 1/100, loss: 0.000566298433114\n",
      "Step 36 (36) @ Episode 1/100, loss: 0.000750173116103\n",
      "Step 37 (37) @ Episode 1/100, loss: 0.000619867350906\n",
      "Step 38 (38) @ Episode 1/100, loss: 0.0317391902208\n",
      "Step 39 (39) @ Episode 1/100, loss: 0.000561928492971\n",
      "Step 40 (40) @ Episode 1/100, loss: 0.000478121102788\n",
      "Step 41 (41) @ Episode 1/100, loss: 0.0338934026659\n",
      "Step 42 (42) @ Episode 1/100, loss: 0.000617952144239\n",
      "Step 43 (43) @ Episode 1/100, loss: 0.000695495633408\n",
      "Step 44 (44) @ Episode 1/100, loss: 0.0336796976626\n",
      "Step 45 (45) @ Episode 1/100, loss: 0.000581136438996\n",
      "Step 46 (46) @ Episode 1/100, loss: 0.000506311131176\n",
      "Step 47 (47) @ Episode 1/100, loss: 0.000569730531424\n",
      "Step 48 (48) @ Episode 1/100, loss: 0.00064041983569\n",
      "Step 49 (49) @ Episode 1/100, loss: 0.0319235250354\n",
      "Step 50 (50) @ Episode 1/100, loss: 0.000526303192601\n",
      "Step 51 (51) @ Episode 1/100, loss: 0.000691797235049\n",
      "Step 52 (52) @ Episode 1/100, loss: 0.000653335067909\n",
      "Step 53 (53) @ Episode 1/100, loss: 0.000664103543386\n",
      "Step 54 (54) @ Episode 1/100, loss: 0.000724705634639\n",
      "Step 55 (55) @ Episode 1/100, loss: 0.0338520742953\n",
      "Step 56 (56) @ Episode 1/100, loss: 0.000683942751493\n",
      "Step 57 (57) @ Episode 1/100, loss: 0.000594371929765\n",
      "Step 58 (58) @ Episode 1/100, loss: 0.000594730081502\n",
      "Step 59 (59) @ Episode 1/100, loss: 0.000566964969039\n",
      "Step 60 (60) @ Episode 1/100, loss: 0.000558220024686\n",
      "Step 61 (61) @ Episode 1/100, loss: 0.000644467072561\n",
      "Step 62 (62) @ Episode 1/100, loss: 0.000646697008051\n",
      "Step 63 (63) @ Episode 1/100, loss: 0.000646171276458\n",
      "Step 64 (64) @ Episode 1/100, loss: 0.0320647396147\n",
      "Step 65 (65) @ Episode 1/100, loss: 0.00062000437174\n",
      "Step 66 (66) @ Episode 1/100, loss: 0.00066245207563\n",
      "Step 67 (67) @ Episode 1/100, loss: 0.000645027554128\n",
      "Step 68 (68) @ Episode 1/100, loss: 0.000618467107415\n",
      "Step 69 (69) @ Episode 1/100, loss: 0.000704379868694\n",
      "Step 70 (70) @ Episode 1/100, loss: 0.0316534973681\n",
      "Step 71 (71) @ Episode 1/100, loss: 0.000626917229965\n",
      "Step 72 (72) @ Episode 1/100, loss: 0.000662232283503\n",
      "Step 73 (73) @ Episode 1/100, loss: 0.000662147649564\n",
      "Step 74 (74) @ Episode 1/100, loss: 0.0320291407406\n",
      "Step 75 (75) @ Episode 1/100, loss: 0.0319762006402\n",
      "Step 76 (76) @ Episode 1/100, loss: 0.000635148782749\n",
      "Step 77 (77) @ Episode 1/100, loss: 0.000614535063505\n",
      "Step 78 (78) @ Episode 1/100, loss: 0.000618265708908\n",
      "Step 79 (79) @ Episode 1/100, loss: 0.000526329269633\n",
      "Step 80 (80) @ Episode 1/100, loss: 0.00061366771115\n",
      "Step 81 (81) @ Episode 1/100, loss: 0.000633602379821\n",
      "Step 82 (82) @ Episode 1/100, loss: 0.0338146947324\n",
      "Step 83 (83) @ Episode 1/100, loss: 0.000623724423349\n",
      "Step 84 (84) @ Episode 1/100, loss: 0.000537039653864\n",
      "Step 85 (85) @ Episode 1/100, loss: 0.000609916110989\n",
      "Step 86 (86) @ Episode 1/100, loss: 0.000575490877964\n",
      "Step 87 (87) @ Episode 1/100, loss: 0.000745993223973\n",
      "Step 88 (88) @ Episode 1/100, loss: 0.0647858306766\n",
      "Step 89 (89) @ Episode 1/100, loss: 0.03378033638\n",
      "Step 90 (90) @ Episode 1/100, loss: 0.000626925029792\n",
      "Step 91 (91) @ Episode 1/100, loss: 0.000620535342023\n",
      "Step 92 (92) @ Episode 1/100, loss: 0.000665439467411\n",
      "Step 93 (93) @ Episode 1/100, loss: 0.000639052130282\n",
      "Step 94 (94) @ Episode 1/100, loss: 0.0319464914501\n",
      "Step 95 (95) @ Episode 1/100, loss: 0.00062270113267\n",
      "Step 96 (96) @ Episode 1/100, loss: 0.00058594107395\n",
      "Step 97 (97) @ Episode 1/100, loss: 0.0336120910943\n",
      "Step 98 (98) @ Episode 1/100, loss: 0.000659760728013\n",
      "Step 99 (99) @ Episode 1/100, loss: 0.000580684340093\n",
      "Step 100 (100) @ Episode 1/100, loss: 0.000499821500853\n",
      "Step 101 (101) @ Episode 1/100, loss: 0.0320405885577\n",
      "Step 102 (102) @ Episode 1/100, loss: 0.000657180557027\n",
      "Step 103 (103) @ Episode 1/100, loss: 0.0338248200715\n",
      "Step 104 (104) @ Episode 1/100, loss: 0.000597263569944\n",
      "Step 105 (105) @ Episode 1/100, loss: 0.000703178695403\n",
      "Step 106 (106) @ Episode 1/100, loss: 0.000565778114833\n",
      "Step 107 (107) @ Episode 1/100, loss: 0.00067766464781\n",
      "Step 108 (108) @ Episode 1/100, loss: 0.000597014441155\n",
      "Step 109 (109) @ Episode 1/100, loss: 0.0340402051806\n",
      "Step 110 (110) @ Episode 1/100, loss: 0.000578530074563\n",
      "Step 111 (111) @ Episode 1/100, loss: 0.0319194309413\n",
      "Step 112 (112) @ Episode 1/100, loss: 0.0336966477334\n",
      "Step 113 (113) @ Episode 1/100, loss: 0.000633023038972\n",
      "Step 114 (114) @ Episode 1/100, loss: 0.000548835494556\n",
      "Step 115 (115) @ Episode 1/100, loss: 0.00062217307277\n",
      "Step 116 (116) @ Episode 1/100, loss: 0.000656755175442\n",
      "Step 117 (117) @ Episode 1/100, loss: 0.000593689153902\n",
      "Step 118 (118) @ Episode 1/100, loss: 0.000534660124686\n",
      "Step 119 (119) @ Episode 1/100, loss: 0.000557438819669\n",
      "Step 120 (120) @ Episode 1/100, loss: 0.000643639476039\n",
      "Step 121 (121) @ Episode 1/100, loss: 0.000660017307382\n",
      "Step 122 (122) @ Episode 1/100, loss: 0.000581688131206\n",
      "Step 123 (123) @ Episode 1/100, loss: 0.000633592775557\n",
      "Step 124 (124) @ Episode 1/100, loss: 0.0337409190834\n",
      "Step 125 (125) @ Episode 1/100, loss: 0.000663288170472\n",
      "Step 126 (126) @ Episode 1/100, loss: 0.000575793732423\n",
      "Step 127 (127) @ Episode 1/100, loss: 0.000607580703218\n",
      "Step 128 (128) @ Episode 1/100, loss: 0.000672887428664\n",
      "Step 129 (129) @ Episode 1/100, loss: 0.000608720991295\n",
      "Step 130 (130) @ Episode 1/100, loss: 0.0319918766618\n",
      "Step 131 (131) @ Episode 1/100, loss: 0.000671482179314\n",
      "Step 132 (132) @ Episode 1/100, loss: 0.0336583964527\n",
      "Step 133 (133) @ Episode 1/100, loss: 0.0336831435561\n",
      "Step 134 (134) @ Episode 1/100, loss: 0.000764885451645\n",
      "Step 135 (135) @ Episode 1/100, loss: 0.0319043137133\n",
      "Step 136 (136) @ Episode 1/100, loss: 0.000524351664353\n",
      "Step 137 (137) @ Episode 1/100, loss: 0.000732616055757\n",
      "Step 138 (138) @ Episode 1/100, loss: 0.000686040730216\n",
      "Step 139 (139) @ Episode 1/100, loss: 0.0315600000322\n",
      "Step 140 (140) @ Episode 1/100, loss: 0.000582113629207\n",
      "Step 141 (141) @ Episode 1/100, loss: 0.000685157603584\n",
      "Step 142 (142) @ Episode 1/100, loss: 0.000694125075825\n",
      "Step 143 (143) @ Episode 1/100, loss: 0.000632177921943\n",
      "Step 144 (144) @ Episode 1/100, loss: 0.0315680019557\n",
      "Step 145 (145) @ Episode 1/100, loss: 0.000642469152808\n",
      "Step 146 (146) @ Episode 1/100, loss: 0.000555837526917\n",
      "Step 147 (147) @ Episode 1/100, loss: 0.000722322962247\n",
      "Step 148 (148) @ Episode 1/100, loss: 0.000778066343628\n",
      "Step 149 (149) @ Episode 1/100, loss: 0.000643404200673\n",
      "Step 150 (150) @ Episode 1/100, loss: 0.000662829377688\n",
      "Step 151 (151) @ Episode 1/100, loss: 0.000580337131396\n",
      "Step 152 (152) @ Episode 1/100, loss: 0.000615890836343\n",
      "Step 153 (153) @ Episode 1/100, loss: 0.00056118669454\n",
      "Step 154 (154) @ Episode 1/100, loss: 0.000631133560091\n",
      "Step 155 (155) @ Episode 1/100, loss: 0.000564851448871\n",
      "Step 156 (156) @ Episode 1/100, loss: 0.000685838982463\n",
      "Step 157 (157) @ Episode 1/100, loss: 0.000583454442676\n",
      "Step 158 (158) @ Episode 1/100, loss: 0.000679795397446\n",
      "Step 159 (159) @ Episode 1/100, loss: 0.000603718392085\n",
      "Step 160 (160) @ Episode 1/100, loss: 0.000585583853535\n",
      "Step 161 (161) @ Episode 1/100, loss: 0.000673453498166\n",
      "Step 162 (162) @ Episode 1/100, loss: 0.0337838828564\n",
      "Step 163 (163) @ Episode 1/100, loss: 0.000650073227007\n",
      "Step 164 (164) @ Episode 1/100, loss: 0.000668762600981\n",
      "Step 165 (165) @ Episode 1/100, loss: 0.0337713286281\n",
      "Step 166 (166) @ Episode 1/100, loss: 0.000598679180257\n",
      "Step 167 (167) @ Episode 1/100, loss: 0.000686163664795\n",
      "Step 168 (168) @ Episode 1/100, loss: 0.000606605724897\n",
      "Step 169 (169) @ Episode 1/100, loss: 0.000640129088424\n",
      "Step 170 (170) @ Episode 1/100, loss: 0.000535692088306\n",
      "Step 171 (171) @ Episode 1/100, loss: 0.000706575810909\n",
      "Step 172 (172) @ Episode 1/100, loss: 0.000619202677626\n",
      "Step 173 (173) @ Episode 1/100, loss: 0.000650661590043\n",
      "Step 174 (174) @ Episode 1/100, loss: 0.000686573039275\n",
      "Step 175 (175) @ Episode 1/100, loss: 0.0315817072988\n",
      "Step 176 (176) @ Episode 1/100, loss: 0.000653384719044\n",
      "Step 177 (177) @ Episode 1/100, loss: 0.000652536808047\n",
      "Step 178 (178) @ Episode 1/100, loss: 0.000645056948997\n",
      "Step 179 (179) @ Episode 1/100, loss: 0.0336450636387\n",
      "Step 180 (180) @ Episode 1/100, loss: 0.000567441224121\n",
      "Step 181 (181) @ Episode 1/100, loss: 0.0647851675749\n",
      "Step 182 (182) @ Episode 1/100, loss: 0.000612268864643\n",
      "Step 183 (183) @ Episode 1/100, loss: 0.000531733152457\n",
      "Step 184 (184) @ Episode 1/100, loss: 0.000574628764298\n",
      "Step 185 (185) @ Episode 1/100, loss: 0.0338185802102\n",
      "Step 186 (186) @ Episode 1/100, loss: 0.000593176810071\n",
      "Step 187 (187) @ Episode 1/100, loss: 0.000603819731623\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (187) @ Episode 2/100, loss: None\n",
      "Step 1 (188) @ Episode 2/100, loss: 0.000706276099663\n",
      "Step 2 (189) @ Episode 2/100, loss: 0.000573171477299\n",
      "Step 3 (190) @ Episode 2/100, loss: 0.000608923961408\n",
      "Step 4 (191) @ Episode 2/100, loss: 0.000555876467843\n",
      "Step 5 (192) @ Episode 2/100, loss: 0.000750782433897\n",
      "Step 6 (193) @ Episode 2/100, loss: 0.000700337579474\n",
      "Step 7 (194) @ Episode 2/100, loss: 0.000526005926076\n",
      "Step 8 (195) @ Episode 2/100, loss: 0.00054755032761\n",
      "Step 9 (196) @ Episode 2/100, loss: 0.000620573642664\n",
      "Step 10 (197) @ Episode 2/100, loss: 0.000667792512104\n",
      "Step 11 (198) @ Episode 2/100, loss: 0.000636134704109\n",
      "Step 12 (199) @ Episode 2/100, loss: 0.000758649897762\n",
      "Step 13 (200) @ Episode 2/100, loss: 0.000633942661807\n",
      "Step 14 (201) @ Episode 2/100, loss: 0.000644479645416\n",
      "Step 15 (202) @ Episode 2/100, loss: 0.000574183242861\n",
      "Step 16 (203) @ Episode 2/100, loss: 0.000668053166009\n",
      "Step 17 (204) @ Episode 2/100, loss: 0.000554160447791\n",
      "Step 18 (205) @ Episode 2/100, loss: 0.0336291901767\n",
      "Step 19 (206) @ Episode 2/100, loss: 0.031854275614\n",
      "Step 20 (207) @ Episode 2/100, loss: 0.000589830800891\n",
      "Step 21 (208) @ Episode 2/100, loss: 0.0336827225983\n",
      "Step 22 (209) @ Episode 2/100, loss: 0.000631713424809\n",
      "Step 23 (210) @ Episode 2/100, loss: 0.000742842443287\n",
      "Step 24 (211) @ Episode 2/100, loss: 0.000595031538978\n",
      "Step 25 (212) @ Episode 2/100, loss: 0.0340984798968\n",
      "Step 26 (213) @ Episode 2/100, loss: 0.0317843966186\n",
      "Step 27 (214) @ Episode 2/100, loss: 0.000610399001744\n",
      "Step 28 (215) @ Episode 2/100, loss: 0.000594762270339\n",
      "Step 29 (216) @ Episode 2/100, loss: 0.00062415964203\n",
      "Step 30 (217) @ Episode 2/100, loss: 0.000616414414253\n",
      "Step 31 (218) @ Episode 2/100, loss: 0.000596935453359\n",
      "Step 32 (219) @ Episode 2/100, loss: 0.000578057952225\n",
      "Step 33 (220) @ Episode 2/100, loss: 0.000549213553313\n",
      "Step 34 (221) @ Episode 2/100, loss: 0.00065761822043\n",
      "Step 35 (222) @ Episode 2/100, loss: 0.000613127485849\n",
      "Step 36 (223) @ Episode 2/100, loss: 0.00059829687234\n",
      "Step 37 (224) @ Episode 2/100, loss: 0.033638317138\n",
      "Step 38 (225) @ Episode 2/100, loss: 0.000596245168708\n",
      "Step 39 (226) @ Episode 2/100, loss: 0.000630964816082\n",
      "Step 40 (227) @ Episode 2/100, loss: 0.000666912412271\n",
      "Step 41 (228) @ Episode 2/100, loss: 0.000635415781289\n",
      "Step 42 (229) @ Episode 2/100, loss: 0.000585934496485\n",
      "Step 43 (230) @ Episode 2/100, loss: 0.000591352349147\n",
      "Step 44 (231) @ Episode 2/100, loss: 0.000468654499855\n",
      "Step 45 (232) @ Episode 2/100, loss: 0.000591787742451\n",
      "Step 46 (233) @ Episode 2/100, loss: 0.000596263911575\n",
      "Step 47 (234) @ Episode 2/100, loss: 0.0336402133107\n",
      "Step 48 (235) @ Episode 2/100, loss: 0.000702234741766\n",
      "Step 49 (236) @ Episode 2/100, loss: 0.000391245237552\n",
      "Step 50 (237) @ Episode 2/100, loss: 0.000627452274784\n",
      "Step 51 (238) @ Episode 2/100, loss: 0.000654801493511\n",
      "Step 52 (239) @ Episode 2/100, loss: 0.000544733484276\n",
      "Step 53 (240) @ Episode 2/100, loss: 0.0005505924928\n",
      "Step 54 (241) @ Episode 2/100, loss: 0.0337418057024\n",
      "Step 55 (242) @ Episode 2/100, loss: 0.0339003205299\n",
      "Step 56 (243) @ Episode 2/100, loss: 0.000686057610437\n",
      "Step 57 (244) @ Episode 2/100, loss: 0.0336587801576\n",
      "Step 58 (245) @ Episode 2/100, loss: 0.0318518839777\n",
      "Step 59 (246) @ Episode 2/100, loss: 0.000736311194487\n",
      "Step 60 (247) @ Episode 2/100, loss: 0.0336472131312\n",
      "Step 61 (248) @ Episode 2/100, loss: 0.000596983823925\n",
      "Step 62 (249) @ Episode 2/100, loss: 0.000657636905089\n",
      "Step 63 (250) @ Episode 2/100, loss: 0.000653683266137\n",
      "Step 64 (251) @ Episode 2/100, loss: 0.000606270390563\n",
      "Step 65 (252) @ Episode 2/100, loss: 0.000634379452094\n",
      "Step 66 (253) @ Episode 2/100, loss: 0.000695268274285\n",
      "Step 67 (254) @ Episode 2/100, loss: 0.0336848646402\n",
      "Step 68 (255) @ Episode 2/100, loss: 0.000617919489741\n",
      "Step 69 (256) @ Episode 2/100, loss: 0.000612801522948\n",
      "Step 70 (257) @ Episode 2/100, loss: 0.0666935667396\n",
      "Step 71 (258) @ Episode 2/100, loss: 0.000583498040214\n",
      "Step 72 (259) @ Episode 2/100, loss: 0.0318222530186\n",
      "Step 73 (260) @ Episode 2/100, loss: 0.000730764702894\n",
      "Step 74 (261) @ Episode 2/100, loss: 0.0335258059204\n",
      "Step 75 (262) @ Episode 2/100, loss: 0.000663827755488\n",
      "Step 76 (263) @ Episode 2/100, loss: 0.000605034874752\n",
      "Step 77 (264) @ Episode 2/100, loss: 0.000650146743283\n",
      "Step 78 (265) @ Episode 2/100, loss: 0.0338538959622\n",
      "Step 79 (266) @ Episode 2/100, loss: 0.000673584989272\n",
      "Step 80 (267) @ Episode 2/100, loss: 0.00070027273614\n",
      "Step 81 (268) @ Episode 2/100, loss: 0.000737956957892\n",
      "Step 82 (269) @ Episode 2/100, loss: 0.000565165886655\n",
      "Step 83 (270) @ Episode 2/100, loss: 0.000623304164037\n",
      "Step 84 (271) @ Episode 2/100, loss: 0.0668473541737\n",
      "Step 85 (272) @ Episode 2/100, loss: 0.000720583309885\n",
      "Step 86 (273) @ Episode 2/100, loss: 0.000622729072347\n",
      "Step 87 (274) @ Episode 2/100, loss: 0.000627179280855\n",
      "Step 88 (275) @ Episode 2/100, loss: 0.000598750426434\n",
      "Step 89 (276) @ Episode 2/100, loss: 0.000599610270001\n",
      "Step 90 (277) @ Episode 2/100, loss: 0.000694562797435\n",
      "Step 91 (278) @ Episode 2/100, loss: 0.000744541874155\n",
      "Step 92 (279) @ Episode 2/100, loss: 0.0318634361029\n",
      "Step 93 (280) @ Episode 2/100, loss: 0.00055005971808\n",
      "Step 94 (281) @ Episode 2/100, loss: 0.0317906998098\n",
      "Step 95 (282) @ Episode 2/100, loss: 0.000697016017511\n",
      "Step 96 (283) @ Episode 2/100, loss: 0.0317276418209\n",
      "Step 97 (284) @ Episode 2/100, loss: 0.000587603601161\n",
      "Step 98 (285) @ Episode 2/100, loss: 0.000657038879581\n",
      "Step 99 (286) @ Episode 2/100, loss: 0.000676937284879\n",
      "Step 100 (287) @ Episode 2/100, loss: 0.000602847896516\n",
      "Step 101 (288) @ Episode 2/100, loss: 0.000661872094497\n",
      "Step 102 (289) @ Episode 2/100, loss: 0.000675806717481\n",
      "Step 103 (290) @ Episode 2/100, loss: 0.000675943796523\n",
      "Step 104 (291) @ Episode 2/100, loss: 0.000693320704158\n",
      "Step 105 (292) @ Episode 2/100, loss: 0.000579624786042\n",
      "Step 106 (293) @ Episode 2/100, loss: 0.00053512013983\n",
      "Step 107 (294) @ Episode 2/100, loss: 0.000576799968258\n",
      "Step 108 (295) @ Episode 2/100, loss: 0.0315747447312\n",
      "Step 109 (296) @ Episode 2/100, loss: 0.000615417317022\n",
      "Step 110 (297) @ Episode 2/100, loss: 0.000625620596111\n",
      "Step 111 (298) @ Episode 2/100, loss: 0.000626920489594\n",
      "Step 112 (299) @ Episode 2/100, loss: 0.000670532695949\n",
      "Step 113 (300) @ Episode 2/100, loss: 0.000511376769282\n",
      "Step 114 (301) @ Episode 2/100, loss: 0.000586071284488\n",
      "Step 115 (302) @ Episode 2/100, loss: 0.000653935130686\n",
      "Step 116 (303) @ Episode 2/100, loss: 0.000623158179224\n",
      "Step 117 (304) @ Episode 2/100, loss: 0.000700923555996\n",
      "Step 118 (305) @ Episode 2/100, loss: 0.000573650817387\n",
      "Step 119 (306) @ Episode 2/100, loss: 0.000560583313927\n",
      "Step 120 (307) @ Episode 2/100, loss: 0.0317214094102\n",
      "Step 121 (308) @ Episode 2/100, loss: 0.000684866507072\n",
      "Step 122 (309) @ Episode 2/100, loss: 0.000680370489135\n",
      "Step 123 (310) @ Episode 2/100, loss: 0.000695252791047\n",
      "Step 124 (311) @ Episode 2/100, loss: 0.000707544735633\n",
      "Step 125 (312) @ Episode 2/100, loss: 0.000470900558867\n",
      "Step 126 (313) @ Episode 2/100, loss: 0.000696802919265\n",
      "Step 127 (314) @ Episode 2/100, loss: 0.000749418453779\n",
      "Step 128 (315) @ Episode 2/100, loss: 0.000567167066038\n",
      "Step 129 (316) @ Episode 2/100, loss: 0.000701007258613\n",
      "Step 130 (317) @ Episode 2/100, loss: 0.000518544111401\n",
      "Step 131 (318) @ Episode 2/100, loss: 0.0336103476584\n",
      "Step 132 (319) @ Episode 2/100, loss: 0.000543023808859\n",
      "Step 133 (320) @ Episode 2/100, loss: 0.000676224415656\n",
      "Step 134 (321) @ Episode 2/100, loss: 0.000550664961338\n",
      "Step 135 (322) @ Episode 2/100, loss: 0.000598268467002\n",
      "Step 136 (323) @ Episode 2/100, loss: 0.000643240637146\n",
      "Step 137 (324) @ Episode 2/100, loss: 0.000570562086068\n",
      "Step 138 (325) @ Episode 2/100, loss: 0.00069034146145\n",
      "Step 139 (326) @ Episode 2/100, loss: 0.0645622909069\n",
      "Step 140 (327) @ Episode 2/100, loss: 0.000685715291183\n",
      "Step 141 (328) @ Episode 2/100, loss: 0.000595637888182\n",
      "Step 142 (329) @ Episode 2/100, loss: 0.000701988581568\n",
      "Step 143 (330) @ Episode 2/100, loss: 0.000661329482682\n",
      "Step 144 (331) @ Episode 2/100, loss: 0.000627448607702\n",
      "Step 145 (332) @ Episode 2/100, loss: 0.000566173519474\n",
      "Step 146 (333) @ Episode 2/100, loss: 0.000580392603297\n",
      "Step 147 (334) @ Episode 2/100, loss: 0.000652829534374\n",
      "Step 148 (335) @ Episode 2/100, loss: 0.000760128081311\n",
      "Step 149 (336) @ Episode 2/100, loss: 0.000626724329777\n",
      "Step 150 (337) @ Episode 2/100, loss: 0.0316370762885\n",
      "Step 151 (338) @ Episode 2/100, loss: 0.000670162611641\n",
      "Step 152 (339) @ Episode 2/100, loss: 0.000574877078179\n",
      "Step 153 (340) @ Episode 2/100, loss: 0.00061775452923\n",
      "Step 154 (341) @ Episode 2/100, loss: 0.000677909934893\n",
      "Step 155 (342) @ Episode 2/100, loss: 0.000639555044472\n",
      "Step 156 (343) @ Episode 2/100, loss: 0.033731687814\n",
      "Step 157 (344) @ Episode 2/100, loss: 0.00066719786264\n",
      "Step 158 (345) @ Episode 2/100, loss: 0.000652184593491\n",
      "Step 159 (346) @ Episode 2/100, loss: 0.000654349976685\n",
      "Step 160 (347) @ Episode 2/100, loss: 0.000524414936081\n",
      "Step 161 (348) @ Episode 2/100, loss: 0.00065020279726\n",
      "Step 162 (349) @ Episode 2/100, loss: 0.00067895220127\n",
      "Step 163 (350) @ Episode 2/100, loss: 0.000487571494887\n",
      "Step 164 (351) @ Episode 2/100, loss: 0.000762513605878\n",
      "Step 165 (352) @ Episode 2/100, loss: 0.0669043809175\n",
      "Step 166 (353) @ Episode 2/100, loss: 0.033734228462\n",
      "Step 167 (354) @ Episode 2/100, loss: 0.0336911454797\n",
      "Step 168 (355) @ Episode 2/100, loss: 0.000580038526095\n",
      "Step 169 (356) @ Episode 2/100, loss: 0.000578013132326\n",
      "Step 170 (357) @ Episode 2/100, loss: 0.000723828794435\n",
      "Step 171 (358) @ Episode 2/100, loss: 0.0335829593241\n",
      "Step 172 (359) @ Episode 2/100, loss: 0.000569576513954\n",
      "Step 173 (360) @ Episode 2/100, loss: 0.000612130155787\n",
      "Step 174 (361) @ Episode 2/100, loss: 0.000616474426351\n",
      "Step 175 (362) @ Episode 2/100, loss: 0.00067427783506\n",
      "Step 176 (363) @ Episode 2/100, loss: 0.0666780546308\n",
      "Step 177 (364) @ Episode 2/100, loss: 0.000775941705797\n",
      "Step 178 (365) @ Episode 2/100, loss: 0.000689951935783\n",
      "Step 179 (366) @ Episode 2/100, loss: 0.000603419844992\n",
      "Step 180 (367) @ Episode 2/100, loss: 0.000664875726216\n",
      "Step 181 (368) @ Episode 2/100, loss: 0.000635439064354\n",
      "Step 182 (369) @ Episode 2/100, loss: 0.000738276925404\n",
      "Step 183 (370) @ Episode 2/100, loss: 0.000643636041787\n",
      "Step 184 (371) @ Episode 2/100, loss: 0.00067048589699\n",
      "Step 185 (372) @ Episode 2/100, loss: 0.000745895318687\n",
      "Step 186 (373) @ Episode 2/100, loss: 0.000505861011334\n",
      "Step 187 (374) @ Episode 2/100, loss: 0.000557618564926\n",
      "Step 188 (375) @ Episode 2/100, loss: 0.0336769968271\n",
      "Step 189 (376) @ Episode 2/100, loss: 0.000633772171568\n",
      "Step 190 (377) @ Episode 2/100, loss: 0.000551235629246\n",
      "Step 191 (378) @ Episode 2/100, loss: 0.000674159848131\n",
      "Step 192 (379) @ Episode 2/100, loss: 0.000635542150121\n",
      "Step 193 (380) @ Episode 2/100, loss: 0.000719476141967\n",
      "Step 194 (381) @ Episode 2/100, loss: 0.000752160325646\n",
      "Step 195 (382) @ Episode 2/100, loss: 0.000585712492466\n",
      "Step 196 (383) @ Episode 2/100, loss: 0.000597266829573\n",
      "Step 197 (384) @ Episode 2/100, loss: 0.000648952904157\n",
      "Step 198 (385) @ Episode 2/100, loss: 0.00067277852213\n",
      "Step 199 (386) @ Episode 2/100, loss: 0.000540123903193\n",
      "Step 200 (387) @ Episode 2/100, loss: 0.000631207542028\n",
      "Step 201 (388) @ Episode 2/100, loss: 0.000705212587491\n",
      "Step 202 (389) @ Episode 2/100, loss: 0.000633140967693\n",
      "Step 203 (390) @ Episode 2/100, loss: 0.000668887165375\n",
      "Step 204 (391) @ Episode 2/100, loss: 0.000496679800563\n",
      "Step 205 (392) @ Episode 2/100, loss: 0.000601047941018\n",
      "Step 206 (393) @ Episode 2/100, loss: 0.000656855292618\n",
      "Step 207 (394) @ Episode 2/100, loss: 0.000489339057822\n",
      "Step 208 (395) @ Episode 2/100, loss: 0.0318506881595\n",
      "Step 209 (396) @ Episode 2/100, loss: 0.00064815278165\n",
      "Step 210 (397) @ Episode 2/100, loss: 0.000473209394841\n",
      "Step 211 (398) @ Episode 2/100, loss: 0.000611469033174\n",
      "Step 212 (399) @ Episode 2/100, loss: 0.000650877598673\n",
      "Step 213 (400) @ Episode 2/100, loss: 0.00060322182253\n",
      "Step 214 (401) @ Episode 2/100, loss: 0.000729535182472\n",
      "Step 215 (402) @ Episode 2/100, loss: 0.000754230597522\n",
      "Step 216 (403) @ Episode 2/100, loss: 0.000645521446131\n",
      "Step 217 (404) @ Episode 2/100, loss: 0.000669809873216\n",
      "Step 218 (405) @ Episode 2/100, loss: 0.00054867262952\n",
      "Step 219 (406) @ Episode 2/100, loss: 0.000580636493396\n",
      "Step 220 (407) @ Episode 2/100, loss: 0.00060952687636\n",
      "Step 221 (408) @ Episode 2/100, loss: 0.000548040494323\n",
      "Step 222 (409) @ Episode 2/100, loss: 0.000563987414353\n",
      "Step 223 (410) @ Episode 2/100, loss: 0.000686059473082\n",
      "Step 224 (411) @ Episode 2/100, loss: 0.000725288526155\n",
      "Step 225 (412) @ Episode 2/100, loss: 0.000600607017986\n",
      "Step 226 (413) @ Episode 2/100, loss: 0.000652022892609\n",
      "Step 227 (414) @ Episode 2/100, loss: 0.000724273384549\n",
      "Step 228 (415) @ Episode 2/100, loss: 0.0335822254419\n",
      "Step 229 (416) @ Episode 2/100, loss: 0.000643582490738\n",
      "Step 230 (417) @ Episode 2/100, loss: 0.000707470811903\n",
      "Step 231 (418) @ Episode 2/100, loss: 0.000695094524417\n",
      "Step 232 (419) @ Episode 2/100, loss: 0.000570145901293\n",
      "Step 233 (420) @ Episode 2/100, loss: 0.000775626627728\n",
      "Step 234 (421) @ Episode 2/100, loss: 0.000736410729587\n",
      "Step 235 (422) @ Episode 2/100, loss: 0.00059643923305\n",
      "Step 236 (423) @ Episode 2/100, loss: 0.00057373958407\n",
      "Step 237 (424) @ Episode 2/100, loss: 0.000664953491651\n",
      "Step 238 (425) @ Episode 2/100, loss: 0.0316461250186\n",
      "Step 239 (426) @ Episode 2/100, loss: 0.000619382131845\n",
      "Step 240 (427) @ Episode 2/100, loss: 0.000628569861874\n",
      "Step 241 (428) @ Episode 2/100, loss: 0.000754113483708\n",
      "Step 242 (429) @ Episode 2/100, loss: 0.000694637710694\n",
      "Step 243 (430) @ Episode 2/100, loss: 0.000731572741643\n",
      "Step 244 (431) @ Episode 2/100, loss: 0.000634659430943\n",
      "Step 245 (432) @ Episode 2/100, loss: 0.000704384583514\n",
      "Step 246 (433) @ Episode 2/100, loss: 0.000607721100096\n",
      "Step 247 (434) @ Episode 2/100, loss: 0.000610723684076\n",
      "Step 248 (435) @ Episode 2/100, loss: 0.000729454390239\n",
      "Step 249 (436) @ Episode 2/100, loss: 0.000676084135193\n",
      "Step 250 (437) @ Episode 2/100, loss: 0.000710867287125\n",
      "Step 251 (438) @ Episode 2/100, loss: 0.000716088456102\n",
      "Step 252 (439) @ Episode 2/100, loss: 0.000669788802043\n",
      "Step 253 (440) @ Episode 2/100, loss: 0.000784915522672\n",
      "Step 254 (441) @ Episode 2/100, loss: 0.000653115799651\n",
      "Step 255 (442) @ Episode 2/100, loss: 0.000715962436516\n",
      "Step 256 (443) @ Episode 2/100, loss: 0.000734339933842\n",
      "Step 257 (444) @ Episode 2/100, loss: 0.000726867234334\n",
      "Step 258 (445) @ Episode 2/100, loss: 0.000720525858924\n",
      "Step 259 (446) @ Episode 2/100, loss: 0.000643085921183\n",
      "Step 260 (447) @ Episode 2/100, loss: 0.000660569989122\n",
      "Step 261 (448) @ Episode 2/100, loss: 0.000694156216923\n",
      "Step 262 (449) @ Episode 2/100, loss: 0.000521506532095\n",
      "Step 263 (450) @ Episode 2/100, loss: 0.000670415232889\n",
      "Step 264 (451) @ Episode 2/100, loss: 0.0315669290721\n",
      "Step 265 (452) @ Episode 2/100, loss: 0.000537711195648\n",
      "Step 266 (453) @ Episode 2/100, loss: 0.000623652595095\n",
      "Step 267 (454) @ Episode 2/100, loss: 0.000655302777886\n",
      "Step 268 (455) @ Episode 2/100, loss: 0.000613777316175\n",
      "Step 269 (456) @ Episode 2/100, loss: 0.0650974735618\n",
      "Step 270 (457) @ Episode 2/100, loss: 0.00058718631044\n",
      "Step 271 (458) @ Episode 2/100, loss: 0.00057804537937\n",
      "Step 272 (459) @ Episode 2/100, loss: 0.0340843386948\n",
      "Step 273 (460) @ Episode 2/100, loss: 0.000680433004163\n",
      "Step 274 (461) @ Episode 2/100, loss: 0.000579454354011\n",
      "Step 275 (462) @ Episode 2/100, loss: 0.000684425875079\n",
      "Step 276 (463) @ Episode 2/100, loss: 0.0316281691194\n",
      "Step 277 (464) @ Episode 2/100, loss: 0.000622469233349\n",
      "Step 278 (465) @ Episode 2/100, loss: 0.00046513939742\n",
      "Step 279 (466) @ Episode 2/100, loss: 0.0340515226126\n",
      "Step 280 (467) @ Episode 2/100, loss: 0.000678341835737\n",
      "Step 281 (468) @ Episode 2/100, loss: 0.0336255393922\n",
      "Step 282 (469) @ Episode 2/100, loss: 0.000571353826672\n",
      "Step 283 (470) @ Episode 2/100, loss: 0.000657692085952\n",
      "Step 284 (471) @ Episode 2/100, loss: 0.000606425513979\n",
      "Step 285 (472) @ Episode 2/100, loss: 0.000573465484194\n",
      "Step 286 (473) @ Episode 2/100, loss: 0.000667484768201\n",
      "Step 287 (474) @ Episode 2/100, loss: 0.000659273529891\n",
      "Step 288 (475) @ Episode 2/100, loss: 0.000668432097882\n",
      "Step 289 (476) @ Episode 2/100, loss: 0.000692247063853\n",
      "Step 290 (477) @ Episode 2/100, loss: 0.000712223933078\n",
      "Step 291 (478) @ Episode 2/100, loss: 0.000621415500063\n",
      "Step 292 (479) @ Episode 2/100, loss: 0.000724148470908\n",
      "Step 293 (480) @ Episode 2/100, loss: 0.00062900246121\n",
      "Step 294 (481) @ Episode 2/100, loss: 0.000724155455828\n",
      "Step 295 (482) @ Episode 2/100, loss: 0.000725222926121\n",
      "Step 296 (483) @ Episode 2/100, loss: 0.000607475405559\n",
      "Step 297 (484) @ Episode 2/100, loss: 0.0316867679358\n",
      "Step 298 (485) @ Episode 2/100, loss: 0.000527882424649\n",
      "Step 299 (486) @ Episode 2/100, loss: 0.000530773773789\n",
      "Step 300 (487) @ Episode 2/100, loss: 0.000712143606506\n",
      "Step 301 (488) @ Episode 2/100, loss: 0.00066702690674\n",
      "Step 302 (489) @ Episode 2/100, loss: 0.000597256410401\n",
      "Step 303 (490) @ Episode 2/100, loss: 0.000779643887654\n",
      "Step 304 (491) @ Episode 2/100, loss: 0.000618949416094\n",
      "Step 305 (492) @ Episode 2/100, loss: 0.000726896163542\n",
      "Step 306 (493) @ Episode 2/100, loss: 0.0315763428807\n",
      "Step 307 (494) @ Episode 2/100, loss: 0.000638870697003\n",
      "Step 308 (495) @ Episode 2/100, loss: 0.000523125985637\n",
      "Step 309 (496) @ Episode 2/100, loss: 0.0336381979287\n",
      "Step 310 (497) @ Episode 2/100, loss: 0.000676512252539\n",
      "Step 311 (498) @ Episode 2/100, loss: 0.000646743050311\n",
      "Step 312 (499) @ Episode 2/100, loss: 0.0336686596274\n",
      "Step 313 (500) @ Episode 2/100, loss: 0.00061571795959\n",
      "Step 314 (501) @ Episode 2/100, loss: 0.000740688294172\n",
      "Step 315 (502) @ Episode 2/100, loss: 0.000627990579233\n",
      "Step 316 (503) @ Episode 2/100, loss: 0.00074177182978\n",
      "Step 317 (504) @ Episode 2/100, loss: 0.0337245240808\n",
      "Step 318 (505) @ Episode 2/100, loss: 0.000511868973263\n",
      "Step 319 (506) @ Episode 2/100, loss: 0.000525893061422\n",
      "Step 320 (507) @ Episode 2/100, loss: 0.000687958206981\n",
      "Step 321 (508) @ Episode 2/100, loss: 0.000672927708365\n",
      "Step 322 (509) @ Episode 2/100, loss: 0.00064385443693\n",
      "Step 323 (510) @ Episode 2/100, loss: 0.00059862609487\n",
      "Step 324 (511) @ Episode 2/100, loss: 0.000689253211021\n",
      "Step 325 (512) @ Episode 2/100, loss: 0.000661389320157\n",
      "Step 326 (513) @ Episode 2/100, loss: 0.0339231938124\n",
      "Step 327 (514) @ Episode 2/100, loss: 0.00056815170683\n",
      "Step 328 (515) @ Episode 2/100, loss: 0.000606135814451\n",
      "Step 329 (516) @ Episode 2/100, loss: 0.0338706001639\n",
      "Step 330 (517) @ Episode 2/100, loss: 0.000524508650415\n",
      "Step 331 (518) @ Episode 2/100, loss: 0.0341155566275\n",
      "Step 332 (519) @ Episode 2/100, loss: 0.000612000469118\n",
      "Step 333 (520) @ Episode 2/100, loss: 0.00065488461405\n",
      "Step 334 (521) @ Episode 2/100, loss: 0.000686908722855\n",
      "Step 335 (522) @ Episode 2/100, loss: 0.00051556143444\n",
      "Step 336 (523) @ Episode 2/100, loss: 0.000685652776156\n",
      "Step 337 (524) @ Episode 2/100, loss: 0.000698229065165\n",
      "Step 338 (525) @ Episode 2/100, loss: 0.000542570604011\n",
      "Step 339 (526) @ Episode 2/100, loss: 0.000653363997117\n",
      "Step 340 (527) @ Episode 2/100, loss: 0.000558801519219\n",
      "Step 341 (528) @ Episode 2/100, loss: 0.000693992944434\n",
      "Step 342 (529) @ Episode 2/100, loss: 0.0337511822581\n",
      "Step 343 (530) @ Episode 2/100, loss: 0.000678950804286\n",
      "Step 344 (531) @ Episode 2/100, loss: 0.000601921579801\n",
      "Step 345 (532) @ Episode 2/100, loss: 0.000639518548269\n",
      "Step 346 (533) @ Episode 2/100, loss: 0.00055960501777\n",
      "Step 347 (534) @ Episode 2/100, loss: 0.000696623930708\n",
      "Step 348 (535) @ Episode 2/100, loss: 0.000705038721208\n",
      "Step 349 (536) @ Episode 2/100, loss: 0.000524220406078\n",
      "Step 350 (537) @ Episode 2/100, loss: 0.000666898093186\n",
      "Step 351 (538) @ Episode 2/100, loss: 0.000590598909184\n",
      "Step 352 (539) @ Episode 2/100, loss: 0.0317423343658\n",
      "Step 353 (540) @ Episode 2/100, loss: 0.0646886378527\n",
      "Step 354 (541) @ Episode 2/100, loss: 0.000743100070395\n",
      "Step 355 (542) @ Episode 2/100, loss: 0.000560176617\n",
      "Step 356 (543) @ Episode 2/100, loss: 0.000662799400743\n",
      "Step 357 (544) @ Episode 2/100, loss: 0.000644296291284\n",
      "Step 358 (545) @ Episode 2/100, loss: 0.0337208658457\n",
      "Step 359 (546) @ Episode 2/100, loss: 0.000713587214705\n",
      "Step 360 (547) @ Episode 2/100, loss: 0.000722696771845\n",
      "Step 361 (548) @ Episode 2/100, loss: 0.000670015753713\n",
      "Step 362 (549) @ Episode 2/100, loss: 0.00062021962367\n",
      "Step 363 (550) @ Episode 2/100, loss: 0.0336824953556\n",
      "Step 364 (551) @ Episode 2/100, loss: 0.000685374834575\n",
      "Step 365 (552) @ Episode 2/100, loss: 0.0338928289711\n",
      "Step 366 (553) @ Episode 2/100, loss: 0.000555414124392\n",
      "Step 367 (554) @ Episode 2/100, loss: 0.000631125352811\n",
      "Step 368 (555) @ Episode 2/100, loss: 0.000604331376962\n",
      "Step 369 (556) @ Episode 2/100, loss: 0.000751286861487\n",
      "Step 370 (557) @ Episode 2/100, loss: 0.000467015604954\n",
      "Step 371 (558) @ Episode 2/100, loss: 0.000550745928194\n",
      "Step 372 (559) @ Episode 2/100, loss: 0.000653619645163\n",
      "Step 373 (560) @ Episode 2/100, loss: 0.000647362554446\n",
      "Step 374 (561) @ Episode 2/100, loss: 0.000567915674765\n",
      "Step 375 (562) @ Episode 2/100, loss: 0.000690757995471\n",
      "Step 376 (563) @ Episode 2/100, loss: 0.000530528370291\n",
      "Step 377 (564) @ Episode 2/100, loss: 0.000708582927473\n",
      "Step 378 (565) @ Episode 2/100, loss: 0.000558192608878\n",
      "Step 379 (566) @ Episode 2/100, loss: 0.000556107610464\n",
      "Step 380 (567) @ Episode 2/100, loss: 0.000699368538335\n",
      "Step 381 (568) @ Episode 2/100, loss: 0.0337238572538\n",
      "Step 382 (569) @ Episode 2/100, loss: 0.000579565006774\n",
      "Step 383 (570) @ Episode 2/100, loss: 0.03359330073\n",
      "Step 384 (571) @ Episode 2/100, loss: 0.000545473594684\n",
      "Step 385 (572) @ Episode 2/100, loss: 0.00068704347359\n",
      "Step 386 (573) @ Episode 2/100, loss: 0.000663109763991\n",
      "Step 387 (574) @ Episode 2/100, loss: 0.000640516751446\n",
      "Step 388 (575) @ Episode 2/100, loss: 0.000676885363646\n",
      "Step 389 (576) @ Episode 2/100, loss: 0.000796920794528\n",
      "Step 390 (577) @ Episode 2/100, loss: 0.00064307032153\n",
      "Step 391 (578) @ Episode 2/100, loss: 0.000573763391003\n",
      "Step 392 (579) @ Episode 2/100, loss: 0.000647223612759\n",
      "Step 393 (580) @ Episode 2/100, loss: 0.000618800928351\n",
      "Step 394 (581) @ Episode 2/100, loss: 0.0336980931461\n",
      "Step 395 (582) @ Episode 2/100, loss: 0.000521552865393\n",
      "Step 396 (583) @ Episode 2/100, loss: 0.000608818489127\n",
      "Step 397 (584) @ Episode 2/100, loss: 0.00058293028269\n",
      "Step 398 (585) @ Episode 2/100, loss: 0.000570274831261\n",
      "Step 399 (586) @ Episode 2/100, loss: 0.00071226793807\n",
      "Step 400 (587) @ Episode 2/100, loss: 0.000625104876235\n",
      "Step 401 (588) @ Episode 2/100, loss: 0.000763572228607\n",
      "Step 402 (589) @ Episode 2/100, loss: 0.000786139280535\n",
      "Step 403 (590) @ Episode 2/100, loss: 0.000641237536911\n",
      "Step 404 (591) @ Episode 2/100, loss: 0.000627417233773\n",
      "Step 405 (592) @ Episode 2/100, loss: 0.000582476786803\n",
      "Step 406 (593) @ Episode 2/100, loss: 0.000607582973316\n",
      "Step 407 (594) @ Episode 2/100, loss: 0.000747035373934\n",
      "Step 408 (595) @ Episode 2/100, loss: 0.000663786078803\n",
      "Step 409 (596) @ Episode 2/100, loss: 0.000722715107258\n",
      "Step 410 (597) @ Episode 2/100, loss: 0.000707894738298\n",
      "Step 411 (598) @ Episode 2/100, loss: 0.000710215594154\n",
      "Step 412 (599) @ Episode 2/100, loss: 0.000541463086847\n",
      "Step 413 (600) @ Episode 2/100, loss: 0.0335184969008\n",
      "Step 414 (601) @ Episode 2/100, loss: 0.00057207030477\n",
      "Step 415 (602) @ Episode 2/100, loss: 0.000739901559427\n",
      "Step 416 (603) @ Episode 2/100, loss: 0.0319229066372\n",
      "Step 417 (604) @ Episode 2/100, loss: 0.000732401735149\n",
      "Step 418 (605) @ Episode 2/100, loss: 0.000774158746935\n",
      "Step 419 (606) @ Episode 2/100, loss: 0.0317035876215\n",
      "Step 420 (607) @ Episode 2/100, loss: 0.000556422688533\n",
      "\n",
      "Episode Reward: 4.0\n",
      "Step 0 (607) @ Episode 3/100, loss: None\n",
      "Step 1 (608) @ Episode 3/100, loss: 0.000620609964244\n",
      "Step 2 (609) @ Episode 3/100, loss: 0.000687000516336\n",
      "Step 3 (610) @ Episode 3/100, loss: 0.000623740546871\n",
      "Step 4 (611) @ Episode 3/100, loss: 0.0316440649331\n",
      "Step 5 (612) @ Episode 3/100, loss: 0.000578931998461\n",
      "Step 6 (613) @ Episode 3/100, loss: 0.000684277387336\n",
      "Step 7 (614) @ Episode 3/100, loss: 0.0314501896501\n",
      "Step 8 (615) @ Episode 3/100, loss: 0.000684221973643\n",
      "Step 9 (616) @ Episode 3/100, loss: 0.0312877558172\n",
      "Step 10 (617) @ Episode 3/100, loss: 0.000634847034235\n",
      "Step 11 (618) @ Episode 3/100, loss: 0.000672427704558\n",
      "Step 12 (619) @ Episode 3/100, loss: 0.000634229741991\n",
      "Step 13 (620) @ Episode 3/100, loss: 0.000687868567184\n",
      "Step 14 (621) @ Episode 3/100, loss: 0.000637910445221\n",
      "Step 15 (622) @ Episode 3/100, loss: 0.000692841364071\n",
      "Step 16 (623) @ Episode 3/100, loss: 0.0311624333262\n",
      "Step 17 (624) @ Episode 3/100, loss: 0.000724593293853\n",
      "Step 18 (625) @ Episode 3/100, loss: 0.0338989086449\n",
      "Step 19 (626) @ Episode 3/100, loss: 0.00072764523793\n",
      "Step 20 (627) @ Episode 3/100, loss: 0.0311052724719\n",
      "Step 21 (628) @ Episode 3/100, loss: 0.00070784491254\n",
      "Step 22 (629) @ Episode 3/100, loss: 0.000758983427659\n",
      "Step 23 (630) @ Episode 3/100, loss: 0.0339121967554\n",
      "Step 24 (631) @ Episode 3/100, loss: 0.000713381159585\n",
      "Step 25 (632) @ Episode 3/100, loss: 0.000611009483691\n",
      "Step 26 (633) @ Episode 3/100, loss: 0.000649706460536\n",
      "Step 27 (634) @ Episode 3/100, loss: 0.000673571252264\n",
      "Step 28 (635) @ Episode 3/100, loss: 0.000708047824446\n",
      "Step 29 (636) @ Episode 3/100, loss: 0.000672120368108\n",
      "Step 30 (637) @ Episode 3/100, loss: 0.00058362854179\n",
      "Step 31 (638) @ Episode 3/100, loss: 0.000685660517775\n",
      "Step 32 (639) @ Episode 3/100, loss: 0.0668729320168\n",
      "Step 33 (640) @ Episode 3/100, loss: 0.000606531801168\n",
      "Step 34 (641) @ Episode 3/100, loss: 0.000643029110506\n",
      "Step 35 (642) @ Episode 3/100, loss: 0.000722967612091\n",
      "Step 36 (643) @ Episode 3/100, loss: 0.000707574305125\n",
      "Step 37 (644) @ Episode 3/100, loss: 0.000604727654718\n",
      "Step 38 (645) @ Episode 3/100, loss: 0.000687978172209\n",
      "Step 39 (646) @ Episode 3/100, loss: 0.000686887069605\n",
      "Step 40 (647) @ Episode 3/100, loss: 0.000725101912394\n",
      "Step 41 (648) @ Episode 3/100, loss: 0.000713871559128\n",
      "Step 42 (649) @ Episode 3/100, loss: 0.000574146513827\n",
      "Step 43 (650) @ Episode 3/100, loss: 0.000689592969138\n",
      "Step 44 (651) @ Episode 3/100, loss: 0.000618392601609\n",
      "Step 45 (652) @ Episode 3/100, loss: 0.000690657412633\n",
      "Step 46 (653) @ Episode 3/100, loss: 0.00054797885241\n",
      "Step 47 (654) @ Episode 3/100, loss: 0.000628260488156\n",
      "Step 48 (655) @ Episode 3/100, loss: 0.000736094079912\n",
      "Step 49 (656) @ Episode 3/100, loss: 0.00066489900928\n",
      "Step 50 (657) @ Episode 3/100, loss: 0.000758548965678\n",
      "Step 51 (658) @ Episode 3/100, loss: 0.000789043260738\n",
      "Step 52 (659) @ Episode 3/100, loss: 0.000769489328377\n",
      "Step 53 (660) @ Episode 3/100, loss: 0.000719496398233\n",
      "Step 54 (661) @ Episode 3/100, loss: 0.00066515954677\n",
      "Step 55 (662) @ Episode 3/100, loss: 0.000621327082627\n",
      "Step 56 (663) @ Episode 3/100, loss: 0.0338282175362\n",
      "Step 57 (664) @ Episode 3/100, loss: 0.000704845646396\n",
      "Step 58 (665) @ Episode 3/100, loss: 0.0665853396058\n",
      "Step 59 (666) @ Episode 3/100, loss: 0.000513055711053\n",
      "Step 60 (667) @ Episode 3/100, loss: 0.000687033985741\n",
      "Step 61 (668) @ Episode 3/100, loss: 0.000711675384082\n",
      "Step 62 (669) @ Episode 3/100, loss: 0.00060627568746\n",
      "Step 63 (670) @ Episode 3/100, loss: 0.000629636575468\n",
      "Step 64 (671) @ Episode 3/100, loss: 0.0315266624093\n",
      "Step 65 (672) @ Episode 3/100, loss: 0.000669371569529\n",
      "Step 66 (673) @ Episode 3/100, loss: 0.000611585448496\n",
      "Step 67 (674) @ Episode 3/100, loss: 0.000667610380333\n",
      "Step 68 (675) @ Episode 3/100, loss: 0.000571791664697\n",
      "Step 69 (676) @ Episode 3/100, loss: 0.00066967838211\n",
      "Step 70 (677) @ Episode 3/100, loss: 0.000653718132526\n",
      "Step 71 (678) @ Episode 3/100, loss: 0.000576012651436\n",
      "Step 72 (679) @ Episode 3/100, loss: 0.000637001532596\n",
      "Step 73 (680) @ Episode 3/100, loss: 0.00071444088826\n",
      "Step 74 (681) @ Episode 3/100, loss: 0.000686708604917\n",
      "Step 75 (682) @ Episode 3/100, loss: 0.000592148920987\n",
      "Step 76 (683) @ Episode 3/100, loss: 0.000712022767402\n",
      "Step 77 (684) @ Episode 3/100, loss: 0.000729979248717\n",
      "Step 78 (685) @ Episode 3/100, loss: 0.000740526069421\n",
      "Step 79 (686) @ Episode 3/100, loss: 0.000660005607642\n",
      "Step 80 (687) @ Episode 3/100, loss: 0.000555665232241\n",
      "Step 81 (688) @ Episode 3/100, loss: 0.000679978460539\n",
      "Step 82 (689) @ Episode 3/100, loss: 0.000740686373319\n",
      "Step 83 (690) @ Episode 3/100, loss: 0.000662304693833\n",
      "Step 84 (691) @ Episode 3/100, loss: 0.000572706921957\n",
      "Step 85 (692) @ Episode 3/100, loss: 0.000551703153178\n",
      "Step 86 (693) @ Episode 3/100, loss: 0.000606456887908\n",
      "Step 87 (694) @ Episode 3/100, loss: 0.000679981196299\n",
      "Step 88 (695) @ Episode 3/100, loss: 0.0670410916209\n",
      "Step 89 (696) @ Episode 3/100, loss: 0.000605909852311\n",
      "Step 90 (697) @ Episode 3/100, loss: 0.000550624681637\n",
      "Step 91 (698) @ Episode 3/100, loss: 0.000718531257007\n",
      "Step 92 (699) @ Episode 3/100, loss: 0.0338740758598\n",
      "Step 93 (700) @ Episode 3/100, loss: 0.000652544084005\n",
      "Step 94 (701) @ Episode 3/100, loss: 0.000637766788714\n",
      "Step 95 (702) @ Episode 3/100, loss: 0.000636304961517\n",
      "Step 96 (703) @ Episode 3/100, loss: 0.000653775990941\n",
      "Step 97 (704) @ Episode 3/100, loss: 0.000560031272471\n",
      "Step 98 (705) @ Episode 3/100, loss: 0.000623007188551\n",
      "Step 99 (706) @ Episode 3/100, loss: 0.000655967276543\n",
      "Step 100 (707) @ Episode 3/100, loss: 0.000611508730799\n",
      "Step 101 (708) @ Episode 3/100, loss: 0.000648772460409\n",
      "Step 102 (709) @ Episode 3/100, loss: 0.000638062134385\n",
      "Step 103 (710) @ Episode 3/100, loss: 0.0336800329387\n",
      "Step 104 (711) @ Episode 3/100, loss: 0.000532421050593\n",
      "Step 105 (712) @ Episode 3/100, loss: 0.000548264710233\n",
      "Step 106 (713) @ Episode 3/100, loss: 0.033867187798\n",
      "Step 107 (714) @ Episode 3/100, loss: 0.0669573843479\n",
      "Step 108 (715) @ Episode 3/100, loss: 0.0335834696889\n",
      "Step 109 (716) @ Episode 3/100, loss: 0.000742628239095\n",
      "Step 110 (717) @ Episode 3/100, loss: 0.000750536681153\n",
      "Step 111 (718) @ Episode 3/100, loss: 0.000571887940168\n",
      "Step 112 (719) @ Episode 3/100, loss: 0.000644776388071\n",
      "Step 113 (720) @ Episode 3/100, loss: 0.000723233737517\n",
      "Step 114 (721) @ Episode 3/100, loss: 0.000724005687516\n",
      "Step 115 (722) @ Episode 3/100, loss: 0.000665925908834\n",
      "Step 116 (723) @ Episode 3/100, loss: 0.000644707819447\n",
      "Step 117 (724) @ Episode 3/100, loss: 0.000608242058661\n",
      "Step 118 (725) @ Episode 3/100, loss: 0.000734793255106\n",
      "Step 119 (726) @ Episode 3/100, loss: 0.000478702044347\n",
      "Step 120 (727) @ Episode 3/100, loss: 0.000516342930496\n",
      "Step 121 (728) @ Episode 3/100, loss: 0.000576297170483\n",
      "Step 122 (729) @ Episode 3/100, loss: 0.000642528175376\n",
      "Step 123 (730) @ Episode 3/100, loss: 0.000729200313799\n",
      "Step 124 (731) @ Episode 3/100, loss: 0.000623895903118\n",
      "Step 125 (732) @ Episode 3/100, loss: 0.000592004624195\n",
      "Step 126 (733) @ Episode 3/100, loss: 0.000631455332041\n",
      "Step 127 (734) @ Episode 3/100, loss: 0.000538277206942\n",
      "Step 128 (735) @ Episode 3/100, loss: 0.000591080402955\n",
      "Step 129 (736) @ Episode 3/100, loss: 0.000785964366514\n",
      "Step 130 (737) @ Episode 3/100, loss: 0.000694842077792\n",
      "Step 131 (738) @ Episode 3/100, loss: 0.000657279742882\n",
      "Step 132 (739) @ Episode 3/100, loss: 0.000712798617315\n",
      "Step 133 (740) @ Episode 3/100, loss: 0.0319523476064\n",
      "Step 134 (741) @ Episode 3/100, loss: 0.000566119037103\n",
      "Step 135 (742) @ Episode 3/100, loss: 0.0337683446705\n",
      "Step 136 (743) @ Episode 3/100, loss: 0.000639845384285\n",
      "Step 137 (744) @ Episode 3/100, loss: 0.00064903695602\n",
      "Step 138 (745) @ Episode 3/100, loss: 0.000544854672626\n",
      "Step 139 (746) @ Episode 3/100, loss: 0.000670158304274\n",
      "Step 140 (747) @ Episode 3/100, loss: 0.000616753823124\n",
      "Step 141 (748) @ Episode 3/100, loss: 0.0008135594544\n",
      "Step 142 (749) @ Episode 3/100, loss: 0.000617805984803\n",
      "Step 143 (750) @ Episode 3/100, loss: 0.000667904503644\n",
      "Step 144 (751) @ Episode 3/100, loss: 0.000540321983863\n",
      "Step 145 (752) @ Episode 3/100, loss: 0.000535653554834\n",
      "Step 146 (753) @ Episode 3/100, loss: 0.000755108892918\n",
      "Step 147 (754) @ Episode 3/100, loss: 0.000640215352178\n",
      "Step 148 (755) @ Episode 3/100, loss: 0.000643886975013\n",
      "Step 149 (756) @ Episode 3/100, loss: 0.0316880345345\n",
      "Step 150 (757) @ Episode 3/100, loss: 0.0315272137523\n",
      "Step 151 (758) @ Episode 3/100, loss: 0.000712765846401\n",
      "Step 152 (759) @ Episode 3/100, loss: 0.000646619708277\n",
      "Step 153 (760) @ Episode 3/100, loss: 0.000658785167616\n",
      "Step 154 (761) @ Episode 3/100, loss: 0.000609429960605\n",
      "Step 155 (762) @ Episode 3/100, loss: 0.000629011949059\n",
      "Step 156 (763) @ Episode 3/100, loss: 0.000648839748465\n",
      "Step 157 (764) @ Episode 3/100, loss: 0.000641953782178\n",
      "Step 158 (765) @ Episode 3/100, loss: 0.000633885152638\n",
      "Step 159 (766) @ Episode 3/100, loss: 0.000631195318419\n",
      "Step 160 (767) @ Episode 3/100, loss: 0.000586977577768\n",
      "Step 161 (768) @ Episode 3/100, loss: 0.000631053175312\n",
      "Step 162 (769) @ Episode 3/100, loss: 0.0645441785455\n",
      "Step 163 (770) @ Episode 3/100, loss: 0.000711234693881\n",
      "Step 164 (771) @ Episode 3/100, loss: 0.000611664843746\n",
      "Step 165 (772) @ Episode 3/100, loss: 0.0310507807881\n",
      "Step 166 (773) @ Episode 3/100, loss: 0.000606630230322\n",
      "Step 167 (774) @ Episode 3/100, loss: 0.031010478735\n",
      "Step 168 (775) @ Episode 3/100, loss: 0.0339756943285\n",
      "Step 169 (776) @ Episode 3/100, loss: 0.000772021245211\n",
      "Step 170 (777) @ Episode 3/100, loss: 0.000699425581843\n",
      "Step 171 (778) @ Episode 3/100, loss: 0.000659327022731\n",
      "Step 172 (779) @ Episode 3/100, loss: 0.000776722095907\n",
      "Step 173 (780) @ Episode 3/100, loss: 0.000665916362777\n",
      "Step 174 (781) @ Episode 3/100, loss: 0.000694689631928\n",
      "Step 175 (782) @ Episode 3/100, loss: 0.000642583763693\n",
      "Step 176 (783) @ Episode 3/100, loss: 0.000729707709979\n",
      "Step 177 (784) @ Episode 3/100, loss: 0.000659115728922\n",
      "Step 178 (785) @ Episode 3/100, loss: 0.0007171331672\n",
      "Step 179 (786) @ Episode 3/100, loss: 0.000674821611028\n",
      "Step 180 (787) @ Episode 3/100, loss: 0.0337511748075\n",
      "Step 181 (788) @ Episode 3/100, loss: 0.000590985990129\n",
      "Step 182 (789) @ Episode 3/100, loss: 0.000784011674114\n",
      "Step 183 (790) @ Episode 3/100, loss: 0.000666348147206\n",
      "Step 184 (791) @ Episode 3/100, loss: 0.0337577536702\n",
      "Step 185 (792) @ Episode 3/100, loss: 0.00063681055326\n",
      "Step 186 (793) @ Episode 3/100, loss: 0.000495388114359\n",
      "Step 187 (794) @ Episode 3/100, loss: 0.000726696453057\n",
      "Step 188 (795) @ Episode 3/100, loss: 0.000661832396872\n",
      "Step 189 (796) @ Episode 3/100, loss: 0.000635294534732\n",
      "Step 190 (797) @ Episode 3/100, loss: 0.000633306452073\n",
      "Step 191 (798) @ Episode 3/100, loss: 0.000654372270219\n",
      "Step 192 (799) @ Episode 3/100, loss: 0.000593973440118\n",
      "Step 193 (800) @ Episode 3/100, loss: 0.000644197803922\n",
      "Step 194 (801) @ Episode 3/100, loss: 0.000542024034075\n",
      "Step 195 (802) @ Episode 3/100, loss: 0.000589183007833\n",
      "Step 196 (803) @ Episode 3/100, loss: 0.000493303639814\n",
      "Step 197 (804) @ Episode 3/100, loss: 0.000671603484079\n",
      "Step 198 (805) @ Episode 3/100, loss: 0.000574789592065\n",
      "Step 199 (806) @ Episode 3/100, loss: 0.0006041910965\n",
      "Step 200 (807) @ Episode 3/100, loss: 0.000681161647663\n",
      "Step 201 (808) @ Episode 3/100, loss: 0.000697222538292\n",
      "Step 202 (809) @ Episode 3/100, loss: 0.000656571181025\n",
      "Step 203 (810) @ Episode 3/100, loss: 0.000558469328098\n",
      "Step 204 (811) @ Episode 3/100, loss: 0.000706230406649\n",
      "Step 205 (812) @ Episode 3/100, loss: 0.000742612406611\n",
      "Step 206 (813) @ Episode 3/100, loss: 0.000584036170039\n",
      "Step 207 (814) @ Episode 3/100, loss: 0.000661792466417\n",
      "Step 208 (815) @ Episode 3/100, loss: 0.000612215721048\n",
      "Step 209 (816) @ Episode 3/100, loss: 0.0006433124654\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 0 (816) @ Episode 4/100, loss: None\n",
      "Step 1 (817) @ Episode 4/100, loss: 0.000633446732536\n",
      "Step 2 (818) @ Episode 4/100, loss: 0.000686229555868\n",
      "Step 3 (819) @ Episode 4/100, loss: 0.00062162487302\n",
      "Step 4 (820) @ Episode 4/100, loss: 0.000610173156019\n",
      "Step 5 (821) @ Episode 4/100, loss: 0.032033264637\n",
      "Step 6 (822) @ Episode 4/100, loss: 0.0338161922991\n",
      "Step 7 (823) @ Episode 4/100, loss: 0.000675886811223\n",
      "Step 8 (824) @ Episode 4/100, loss: 0.000636031152681\n",
      "Step 9 (825) @ Episode 4/100, loss: 0.0648808702826\n",
      "Step 10 (826) @ Episode 4/100, loss: 0.0310727152973\n",
      "Step 11 (827) @ Episode 4/100, loss: 0.000770646031015\n",
      "Step 12 (828) @ Episode 4/100, loss: 0.000837603409309\n",
      "Step 13 (829) @ Episode 4/100, loss: 0.0338015295565\n",
      "Step 14 (830) @ Episode 4/100, loss: 0.034131474793\n",
      "Step 15 (831) @ Episode 4/100, loss: 0.000768390716985\n",
      "Step 16 (832) @ Episode 4/100, loss: 0.000648251385428\n",
      "Step 17 (833) @ Episode 4/100, loss: 0.00079646561062\n",
      "Step 18 (834) @ Episode 4/100, loss: 0.000655264069792\n",
      "Step 19 (835) @ Episode 4/100, loss: 0.000645716150757\n",
      "Step 20 (836) @ Episode 4/100, loss: 0.000675269926433\n",
      "Step 21 (837) @ Episode 4/100, loss: 0.0312565788627\n",
      "Step 22 (838) @ Episode 4/100, loss: 0.000788305420429\n",
      "Step 23 (839) @ Episode 4/100, loss: 0.000669306144118\n",
      "Step 24 (840) @ Episode 4/100, loss: 0.000681040808558\n",
      "Step 25 (841) @ Episode 4/100, loss: 0.0336486585438\n",
      "Step 26 (842) @ Episode 4/100, loss: 0.000711605302058\n",
      "Step 27 (843) @ Episode 4/100, loss: 0.00082601129543\n",
      "Step 28 (844) @ Episode 4/100, loss: 0.000694010872394\n",
      "Step 29 (845) @ Episode 4/100, loss: 0.000609215348959\n",
      "Step 30 (846) @ Episode 4/100, loss: 0.000600755913183\n",
      "Step 31 (847) @ Episode 4/100, loss: 0.000671725138091\n",
      "Step 32 (848) @ Episode 4/100, loss: 0.000736082205549\n",
      "Step 33 (849) @ Episode 4/100, loss: 0.000693631707691\n",
      "Step 34 (850) @ Episode 4/100, loss: 0.00068276154343\n",
      "Step 35 (851) @ Episode 4/100, loss: 0.000583524582908\n",
      "Step 36 (852) @ Episode 4/100, loss: 0.000603631371632\n",
      "Step 37 (853) @ Episode 4/100, loss: 0.000720146112144\n",
      "Step 38 (854) @ Episode 4/100, loss: 0.000576166668907\n",
      "Step 39 (855) @ Episode 4/100, loss: 0.000622829189524\n",
      "Step 40 (856) @ Episode 4/100, loss: 0.00064178428147\n",
      "Step 41 (857) @ Episode 4/100, loss: 0.000693368492648\n",
      "Step 42 (858) @ Episode 4/100, loss: 0.0316046997905\n",
      "Step 43 (859) @ Episode 4/100, loss: 0.000657150172628\n",
      "Step 44 (860) @ Episode 4/100, loss: 0.000665414030664\n",
      "Step 45 (861) @ Episode 4/100, loss: 0.0337400920689\n",
      "Step 46 (862) @ Episode 4/100, loss: 0.000591229880229\n",
      "Step 47 (863) @ Episode 4/100, loss: 0.000647158711217\n",
      "Step 48 (864) @ Episode 4/100, loss: 0.0336530990899\n",
      "Step 49 (865) @ Episode 4/100, loss: 0.000670291949064\n",
      "Step 50 (866) @ Episode 4/100, loss: 0.000468961778097\n",
      "Step 51 (867) @ Episode 4/100, loss: 0.000572405289859\n",
      "Step 52 (868) @ Episode 4/100, loss: 0.000721952295862\n",
      "Step 53 (869) @ Episode 4/100, loss: 0.03384514153\n",
      "Step 54 (870) @ Episode 4/100, loss: 0.00054493767675\n",
      "Step 55 (871) @ Episode 4/100, loss: 0.0314412750304\n",
      "Step 56 (872) @ Episode 4/100, loss: 0.000745447352529\n",
      "Step 57 (873) @ Episode 4/100, loss: 0.000618891906925\n",
      "Step 58 (874) @ Episode 4/100, loss: 0.000706486986019\n",
      "Step 59 (875) @ Episode 4/100, loss: 0.0312856063247\n",
      "Step 60 (876) @ Episode 4/100, loss: 0.000754574197344\n",
      "Step 61 (877) @ Episode 4/100, loss: 0.000716074486263\n",
      "Step 62 (878) @ Episode 4/100, loss: 0.000635656702798\n",
      "Step 63 (879) @ Episode 4/100, loss: 0.0338920392096\n",
      "Step 64 (880) @ Episode 4/100, loss: 0.000670821988024\n",
      "Step 65 (881) @ Episode 4/100, loss: 0.000677353935316\n",
      "Step 66 (882) @ Episode 4/100, loss: 0.000642305007204\n",
      "Step 67 (883) @ Episode 4/100, loss: 0.000663608079776\n",
      "Step 68 (884) @ Episode 4/100, loss: 0.000759448797908\n",
      "Step 69 (885) @ Episode 4/100, loss: 0.000588936556596\n",
      "Step 70 (886) @ Episode 4/100, loss: 0.000615888973698\n",
      "Step 71 (887) @ Episode 4/100, loss: 0.000689009553753\n",
      "Step 72 (888) @ Episode 4/100, loss: 0.000578782870434\n",
      "Step 73 (889) @ Episode 4/100, loss: 0.0338045693934\n",
      "Step 74 (890) @ Episode 4/100, loss: 0.000680980214383\n",
      "Step 75 (891) @ Episode 4/100, loss: 0.000687693129294\n",
      "Step 76 (892) @ Episode 4/100, loss: 0.000646712025627\n",
      "Step 77 (893) @ Episode 4/100, loss: 0.0340404510498\n",
      "Step 78 (894) @ Episode 4/100, loss: 0.0314830951393\n",
      "Step 79 (895) @ Episode 4/100, loss: 0.000626998720691\n",
      "Step 80 (896) @ Episode 4/100, loss: 0.000660010031424\n",
      "Step 81 (897) @ Episode 4/100, loss: 0.000647727632895\n",
      "Step 82 (898) @ Episode 4/100, loss: 0.000687439693138\n",
      "Step 83 (899) @ Episode 4/100, loss: 0.000635850534309\n",
      "Step 84 (900) @ Episode 4/100, loss: 0.000718211231288\n",
      "Step 85 (901) @ Episode 4/100, loss: 0.000672585330904\n",
      "Step 86 (902) @ Episode 4/100, loss: 0.000629861606285\n",
      "Step 87 (903) @ Episode 4/100, loss: 0.000642514321953\n",
      "Step 88 (904) @ Episode 4/100, loss: 0.000649754889309\n",
      "Step 89 (905) @ Episode 4/100, loss: 0.000670817331411\n",
      "Step 90 (906) @ Episode 4/100, loss: 0.000577936880291\n",
      "Step 91 (907) @ Episode 4/100, loss: 0.000736521207727\n",
      "Step 92 (908) @ Episode 4/100, loss: 0.000578643230256\n",
      "Step 93 (909) @ Episode 4/100, loss: 0.000582077773288\n",
      "Step 94 (910) @ Episode 4/100, loss: 0.000648533343337\n",
      "Step 95 (911) @ Episode 4/100, loss: 0.000639033969492\n",
      "Step 96 (912) @ Episode 4/100, loss: 0.0338980071247\n",
      "Step 97 (913) @ Episode 4/100, loss: 0.000703798199538\n",
      "Step 98 (914) @ Episode 4/100, loss: 0.0340631864965\n",
      "Step 99 (915) @ Episode 4/100, loss: 0.000686757673975\n",
      "Step 100 (916) @ Episode 4/100, loss: 0.0337412618101\n",
      "Step 101 (917) @ Episode 4/100, loss: 0.000626679509878\n",
      "Step 102 (918) @ Episode 4/100, loss: 0.000725176243577\n",
      "Step 103 (919) @ Episode 4/100, loss: 0.000693381763995\n",
      "Step 104 (920) @ Episode 4/100, loss: 0.000501058297232\n",
      "Step 105 (921) @ Episode 4/100, loss: 0.000632482755464\n",
      "Step 106 (922) @ Episode 4/100, loss: 0.000610054819845\n",
      "Step 107 (923) @ Episode 4/100, loss: 0.000563068198971\n",
      "Step 108 (924) @ Episode 4/100, loss: 0.000598404905759\n",
      "Step 109 (925) @ Episode 4/100, loss: 0.0316663607955\n",
      "Step 110 (926) @ Episode 4/100, loss: 0.000624101317953\n",
      "Step 111 (927) @ Episode 4/100, loss: 0.000763827934861\n",
      "Step 112 (928) @ Episode 4/100, loss: 0.0311059262604\n",
      "Step 113 (929) @ Episode 4/100, loss: 0.000749484694097\n",
      "Step 114 (930) @ Episode 4/100, loss: 0.0671537965536\n",
      "Step 115 (931) @ Episode 4/100, loss: 0.000727299018763\n",
      "Step 116 (932) @ Episode 4/100, loss: 0.0310814324766\n",
      "Step 117 (933) @ Episode 4/100, loss: 0.000842350069433\n",
      "Step 118 (934) @ Episode 4/100, loss: 0.034001365304\n",
      "Step 119 (935) @ Episode 4/100, loss: 0.000802082766313\n",
      "Step 120 (936) @ Episode 4/100, loss: 0.0339792855084\n",
      "Step 121 (937) @ Episode 4/100, loss: 0.000729237915948\n",
      "Step 122 (938) @ Episode 4/100, loss: 0.000618275837041\n",
      "Step 123 (939) @ Episode 4/100, loss: 0.0338697321713\n",
      "Step 124 (940) @ Episode 4/100, loss: 0.000556260871235\n",
      "Step 125 (941) @ Episode 4/100, loss: 0.00069258431904\n",
      "Step 126 (942) @ Episode 4/100, loss: 0.000680980912875\n",
      "Step 127 (943) @ Episode 4/100, loss: 0.000779064372182\n",
      "Step 128 (944) @ Episode 4/100, loss: 0.000744785007555\n",
      "Step 129 (945) @ Episode 4/100, loss: 0.000717642134987\n",
      "Step 130 (946) @ Episode 4/100, loss: 0.000590147334151\n",
      "Step 131 (947) @ Episode 4/100, loss: 0.000680707918946\n",
      "Step 132 (948) @ Episode 4/100, loss: 0.0313347987831\n",
      "Step 133 (949) @ Episode 4/100, loss: 0.000733388762455\n",
      "Step 134 (950) @ Episode 4/100, loss: 0.000697475101333\n",
      "Step 135 (951) @ Episode 4/100, loss: 0.000575107405894\n",
      "Step 136 (952) @ Episode 4/100, loss: 0.000601616338827\n",
      "Step 137 (953) @ Episode 4/100, loss: 0.00068789004581\n",
      "Step 138 (954) @ Episode 4/100, loss: 0.000590185634792\n",
      "Step 139 (955) @ Episode 4/100, loss: 0.000680168508552\n",
      "Step 140 (956) @ Episode 4/100, loss: 0.0670363306999\n",
      "Step 141 (957) @ Episode 4/100, loss: 0.000601849402301\n",
      "Step 142 (958) @ Episode 4/100, loss: 0.0313689783216\n",
      "Step 143 (959) @ Episode 4/100, loss: 0.000706209917553\n",
      "Step 144 (960) @ Episode 4/100, loss: 0.0338651202619\n",
      "Step 145 (961) @ Episode 4/100, loss: 0.000654393108562\n",
      "Step 146 (962) @ Episode 4/100, loss: 0.000730393454432\n",
      "Step 147 (963) @ Episode 4/100, loss: 0.000712858163752\n",
      "Step 148 (964) @ Episode 4/100, loss: 0.000686391722411\n",
      "Step 149 (965) @ Episode 4/100, loss: 0.000673442904372\n",
      "Step 150 (966) @ Episode 4/100, loss: 0.000606539135333\n",
      "Step 151 (967) @ Episode 4/100, loss: 0.000721725285985\n",
      "Step 152 (968) @ Episode 4/100, loss: 0.000597280799411\n",
      "Step 153 (969) @ Episode 4/100, loss: 0.000658873817883\n",
      "Step 154 (970) @ Episode 4/100, loss: 0.00082268670667\n",
      "Step 155 (971) @ Episode 4/100, loss: 0.000685867504217\n",
      "Step 156 (972) @ Episode 4/100, loss: 0.000584789551795\n",
      "Step 157 (973) @ Episode 4/100, loss: 0.000596576195676\n",
      "Step 158 (974) @ Episode 4/100, loss: 0.000666486041155\n",
      "Step 159 (975) @ Episode 4/100, loss: 0.000658917473629\n",
      "Step 160 (976) @ Episode 4/100, loss: 0.000692108820658\n",
      "Step 161 (977) @ Episode 4/100, loss: 0.000625376473181\n",
      "Step 162 (978) @ Episode 4/100, loss: 0.000627018860541\n",
      "Step 163 (979) @ Episode 4/100, loss: 0.000665213912725\n",
      "Step 164 (980) @ Episode 4/100, loss: 0.0335306860507\n",
      "Step 165 (981) @ Episode 4/100, loss: 0.000696476316079\n",
      "Step 166 (982) @ Episode 4/100, loss: 0.000700830190908\n",
      "Step 167 (983) @ Episode 4/100, loss: 0.00060162943555\n",
      "Step 168 (984) @ Episode 4/100, loss: 0.000623111613095\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-65269963b0bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                                     \u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                     \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                     batch_size=32):\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-182f5805935e>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVALID_ACTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-40330ee335ba>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, sess, state)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mprocessed\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mstate\u001b[0m \u001b[0mrepresenting\u001b[0m \u001b[0mgrayscale\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \"\"\"\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=100,\n",
    "                                    replay_memory_size=50000,\n",
    "                                    replay_memory_init_size=5000,\n",
    "                                    update_target_estimator_every=1000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=50000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
